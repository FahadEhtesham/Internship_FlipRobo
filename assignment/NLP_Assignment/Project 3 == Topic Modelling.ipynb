{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling\n",
    "\n",
    "\n",
    "Collect randomly as many as news content from different fields for training. (thru any form, web scrape, pdf, newspaper etc.)\n",
    "\n",
    "\n",
    "Objective:-Build topic modelling to understand the topics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Imported the package Newspaper for scrapping the newspaper articles</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import newspaper\n",
    " \n",
    "#Using Times of India for scrapping the news    \n",
    "site = newspaper.build(\"https://timesofindia.indiatimes.com/news\", memoize_articles=False)  \n",
    " \n",
    "# get list of article URLs\n",
    "#site.article_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_article = site.articles[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<newspaper.article.Article at 0x1c785c1cb38>,\n",
       " <newspaper.article.Article at 0x1c785c1c780>,\n",
       " <newspaper.article.Article at 0x1c7a55459b0>,\n",
       " <newspaper.article.Article at 0x1c785c2c518>,\n",
       " <newspaper.article.Article at 0x1c785b872b0>,\n",
       " <newspaper.article.Article at 0x1c785b87be0>,\n",
       " <newspaper.article.Article at 0x1c785b87e48>,\n",
       " <newspaper.article.Article at 0x1c785c2cb70>,\n",
       " <newspaper.article.Article at 0x1c787ccea90>,\n",
       " <newspaper.article.Article at 0x1c785c2c588>,\n",
       " <newspaper.article.Article at 0x1c785c2c7b8>,\n",
       " <newspaper.article.Article at 0x1c785c2cf28>,\n",
       " <newspaper.article.Article at 0x1c785c2c9e8>,\n",
       " <newspaper.article.Article at 0x1c785c2c6a0>,\n",
       " <newspaper.article.Article at 0x1c7a44d74a8>,\n",
       " <newspaper.article.Article at 0x1c785c2c278>,\n",
       " <newspaper.article.Article at 0x1c785c2ca90>,\n",
       " <newspaper.article.Article at 0x1c7a44d7860>,\n",
       " <newspaper.article.Article at 0x1c7a44d7fd0>,\n",
       " <newspaper.article.Article at 0x1c785b87d30>,\n",
       " <newspaper.article.Article at 0x1c785bb6ef0>,\n",
       " <newspaper.article.Article at 0x1c7a44d70b8>,\n",
       " <newspaper.article.Article at 0x1c7a44d7438>,\n",
       " <newspaper.article.Article at 0x1c7a44d7128>,\n",
       " <newspaper.article.Article at 0x1c785c4d5f8>,\n",
       " <newspaper.article.Article at 0x1c787cb94a8>,\n",
       " <newspaper.article.Article at 0x1c787cb9630>,\n",
       " <newspaper.article.Article at 0x1c787cb9c50>,\n",
       " <newspaper.article.Article at 0x1c7a551d390>,\n",
       " <newspaper.article.Article at 0x1c785c485c0>,\n",
       " <newspaper.article.Article at 0x1c785c483c8>,\n",
       " <newspaper.article.Article at 0x1c785c48588>,\n",
       " <newspaper.article.Article at 0x1c785c48240>,\n",
       " <newspaper.article.Article at 0x1c787cb9a20>,\n",
       " <newspaper.article.Article at 0x1c787cb9e48>,\n",
       " <newspaper.article.Article at 0x1c785c48a20>,\n",
       " <newspaper.article.Article at 0x1c785c486a0>,\n",
       " <newspaper.article.Article at 0x1c785c2cf98>,\n",
       " <newspaper.article.Article at 0x1c7a44ecc50>,\n",
       " <newspaper.article.Article at 0x1c7a44ec8d0>,\n",
       " <newspaper.article.Article at 0x1c7a44eca58>,\n",
       " <newspaper.article.Article at 0x1c7a552a630>,\n",
       " <newspaper.article.Article at 0x1c785c1cb00>,\n",
       " <newspaper.article.Article at 0x1c785c0d7f0>,\n",
       " <newspaper.article.Article at 0x1c785c0d320>,\n",
       " <newspaper.article.Article at 0x1c785c0d6a0>,\n",
       " <newspaper.article.Article at 0x1c7a44e6d30>,\n",
       " <newspaper.article.Article at 0x1c785bc5b70>,\n",
       " <newspaper.article.Article at 0x1c785bc52b0>,\n",
       " <newspaper.article.Article at 0x1c785bc5908>]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the articles\n",
    "article_list=[]\n",
    "for i in site_article:\n",
    "    i.download()\n",
    "    i.parse()\n",
    "    article_list.append(i.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Printing the sample news </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The indiatimes.com privacy policy has been updated to align with the new data regulations in European Union. Please review and accept these changes below to continue using the website. We use cookies to ensure the best experience for you on our website.'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_list[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preprocessing\n",
    "#Tokenization\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "'''\n",
    "Loading gensim and nltk libraries\n",
    "'''\n",
    "#!pip install gensim\n",
    "import gensim\n",
    "\n",
    "# Converts into tokens (Alternative to word_tokenize)\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "#from nltk.stem import *\n",
    "import numpy as np\n",
    "np.random.seed(400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizer Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordNetLemmatizer().lemmatize('runs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Words</th>\n",
       "      <th>Lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>caresses</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>flies</td>\n",
       "      <td>fly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>dies</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>mules</td>\n",
       "      <td>mules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>denied</td>\n",
       "      <td>deny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>died</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>agreed</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>owned</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>humbled</td>\n",
       "      <td>humble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>sized</td>\n",
       "      <td>size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>meeting</td>\n",
       "      <td>meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>stating</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>siezing</td>\n",
       "      <td>siezing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>itemization</td>\n",
       "      <td>itemization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>sensational</td>\n",
       "      <td>sensational</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>traditional</td>\n",
       "      <td>traditional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>reference</td>\n",
       "      <td>reference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>colonizer</td>\n",
       "      <td>colonizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>plotted</td>\n",
       "      <td>plot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Original Words        Lemma\n",
       "0        caresses       caress\n",
       "1           flies          fly\n",
       "2            dies          die\n",
       "3           mules        mules\n",
       "4          denied         deny\n",
       "5            died          die\n",
       "6          agreed        agree\n",
       "7           owned          own\n",
       "8         humbled       humble\n",
       "9           sized         size\n",
       "10        meeting         meet\n",
       "11        stating        state\n",
       "12        siezing      siezing\n",
       "13    itemization  itemization\n",
       "14    sensational  sensational\n",
       "15    traditional  traditional\n",
       "16      reference    reference\n",
       "17      colonizer    colonizer\n",
       "18        plotted         plot"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing the stemming part before preprocessing. This should be changing any plural into singular word\n",
    "import pandas as pd\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "original_words=['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', \n",
    "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n",
    "           'traditional', 'reference', 'colonizer','plotted']\n",
    "\n",
    "singles= [WordNetLemmatizer().lemmatize(plural,pos='v') for plural in original_words]\n",
    "\n",
    "pd.DataFrame(data={'Original Words':original_words, 'Lemma':singles})\n",
    "\n",
    "#Stemma is not performing well \n",
    "#singles= [stemmer.stem(plural) for plural in original_words]\n",
    "#singles2= [stemmer.stem(plural2) for plural2 in singles]\n",
    "#stemma = pd.DataFrame(data={'Lemma':singles, 'Stemmed':singles2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing function for the entire dataset\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text,pos='v'))\n",
    "\n",
    "#Tokenize and Lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token)>3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Document: \n",
      "['This', 'disk', 'has', 'failed', 'many', 'times.', 'I', 'would', 'like', 'to', 'get', 'it', 'replaced.']\n",
      "\n",
      "\n",
      "Tokenized and lemmatized document: \n",
      "['disk', 'fail', 'time', 'like', 'replac']\n"
     ]
    }
   ],
   "source": [
    "#preview a document before preprocessing\n",
    "\n",
    "doc_sample = 'This disk has failed many times. I would like to get it replaced.'\n",
    "\n",
    "print(\"Original Document: \")\n",
    "words=[]\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print(\"\\n\\nTokenized and lemmatized document: \")\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = []\n",
    "\n",
    "for doc in article_list:\n",
    "    processed_docs.append(preprocess(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['dhoni', 'genius', 'say', 'curran', 'england', 'rounder', 'curran', 'bat', 'ahead', 'mahendra', 'singh', 'dhoni', 'chennai', 'super', 'king', 'mumbai', 'indian', 'open', 'fixtur', 'indian', 'premier', 'leagu', 'saturday', 'england', 'rounder', 'leav', 'surpris', 'genius', 'skipper', 'qaida', 'cell', 'plot', 'hit', 'hold', 'nation', 'investig', 'agenc', 'saturday', 'bust', 'pakistan', 'sponsor', 'qaida', 'modul', 'oper', 'west', 'bengal', 'kerala', 'arrest', 'person', 'plan', 'carri', 'terrorist', 'strike', 'india', 'near', 'futur', 'sophist', 'weapon', 'arrang', 'qaida', 'handler', 'hamza', 'base', 'peshawar', 'pakistan', 'baseless', 'kashyap', 'sexual', 'abus', 'alleg', 'anurag', 'kashyap', 'take', 'twitter', 'respond', 'payal', 'ghosh', 'sexual', 'harass', 'accus', 'call', 'baseless', 'write', 'water', 'power', 'bill', 'act', 'input', 'receiv', 'public', 'outreach', 'take', 'charg', 'month', 'lieuten', 'governor', 'manoj', 'sinha', 'saturday', 'unveil', 'crore', 'econom', 'packag', 'ensur', 'money', 'peopl', 'pocket', 'help', 'fledgl', 'union', 'territori', 'beat', 'busi', 'sector', 'bounc', 'quick', 'creat', 'job', 'ncert', 'offici', 'involv', 'book', 'piraci', 'document', 'intern', 'inquiri', 'report', 'ncert', 'submit', 'ministri', 'educ', 'transcript', 'phone', 'convers', 'messag', 'copi', 'reveal', 'council', 'whistleblow', 'alert', 'day', 'initi', 'action', 'intervent', 'secretari', 'call', 'ministri', 'trump', 'back', 'tiktok', 'deal', 'oracl', 'walmart', 'presid', 'donald', 'trump', 'tell', 'report', 'white', 'hous', 'back', 'deal', 'oracl', 'walmart', 'creation', 'compani', 'assum', 'tiktok', 'oper', 'trump', 'say', 'tiktok', 'compani', 'total', 'control', 'oracl', 'walmart', 'control', 'oracl', 'walmart', 'govt', 'press', 'ahead', 'labour', 'reform', 'govern', 'saturday', 'introduc', 'bill', 'amend', 'consolid', 'labour', 'law', 'expand', 'social', 'secur', 'includ', 'migrant', 'economi', 'worker', 'seek', 'provid', 'greater', 'flexibl', 'busi', 'hire', 'employe', 'teach', 'post', 'govt', 'school', 'vacant', 'total', 'teach', 'post', 'govern', 'school', 'lie', 'vacant', 'present', 'term', 'absolut', 'number', 'largest', 'vacanc', 'bihar', 'lakh', 'follow', 'uttar', 'pradesh', 'lakh', 'percentag', 'wise', 'sikkim', 'vacanc', 'highest', 'vacanc', 'follow', 'jharkhand', 'bihar', 'uttar', 'pradesh', 'chines', 'arrest', 'spi', 'special', 'cell', 'delhi', 'polic', 'arrest', 'chines', 'woman', 'nepales', 'freelanc', 'journalist', 'rajeev', 'sharma', 'claim', 'bust', 'espionag', 'ring', 'book', 'offici', 'secret', 'alleg', 'pass', 'sensit', 'inform', 'chines', 'intellig', 'phase', 'trial', 'oxford', 'vaccin', 'week', 'phase', 'human', 'clinic', 'trial', 'covid', 'vaccin', 'develop', 'oxford', 'univers', 'manufactur', 'begin', 'sassoon', 'general', 'hospit', 'pune', 'week', 'dean', 'state', 'hospit', 'muralidhar', 'tamb', 'say', 'volunt', 'come', 'forward', 'trial', 'volunt', 'administ', 'vaccin', 'dose', 'sushant', 'case', 'actor', 'viscera', 'preserv', 'proper', 'sheer', 'neglig', 'mumbai', 'polic', 'medic', 'board', 'conduct', 'autopsi', 'late', 'bollywood', 'star', 'sushant', 'singh', 'rajput', 'import', 'viscera', 'extract', 'bodi', 'preserv', 'proper', 'arunach', 'forc', 'monitor', 'move', 'area', 'indian', 'defenc', 'forc', 'rais', 'level', 'prepared', 'alert', 'monitor', 'chines', 'activ', 'disput', 'area', 'sensit', 'area', 'theatr', 'arunach', 'pradesh', 'disput', 'area', 'asapila', 'longzu', 'bisa', 'majha', 'upper', 'subansiri', 'district', 'build', 'road', 'bisa', 'point', 'predict', 'extrem', 'heavi', 'rain', 'kerala', 'india', 'meteorolog', 'depart', 'issu', 'alert', 'extrem', 'heavi', 'rainfal', 'kerala', 'district', 'saturday', 'weather', 'condit', 'prevail', 'delhi', 'eleventh', 'consecut', 'depart', 'say', 'pressur', 'brew', 'bengal', 'like', 'trigger', 'rainfal', 'west', 'bengal', 'sunday', 'perman', 'commiss', 'screen', 'process', 'start', 'armi', 'final', 'begin', 'screen', 'women', 'offic', 'grant', 'perman', 'commiss', 'branch', 'accord', 'suprem', 'court', 'direct', 'februari', 'gender', 'bias', 'lakh', 'strong', 'forc', 'special', 'select', 'board', 'constitut', 'screen', 'short', 'servic', 'commiss', 'women', 'offic', 'serv', 'maximum', 'year', 'grant', 'perman', 'commiss', 'armi', 'begin', 'work', 'earlier', 'week', 'dhoni', 'popular', 'surpass', 'sachin', 'koh', 'legendari', 'sunil', 'gavaskar', 'saturday', 'say', 'time', 'world', 'win', 'captain', 'mahendra', 'singh', 'dhoni', 'popular', 'india', 'exceed', 'level', 'fandom', 'great', 'sachin', 'tendulkar', 'virat', 'koh', 'enjoy', 'cricket', 'crazi', 'nation', 'gavaskar', 'india', 'captain', 'comment', 'edit', 'indian', 'premier', 'leagu'], ['use', 'drone', 'drop', 'weapon', 'rajouri', 'pakistan', 'drop', 'weapon', 'cash', 'indian', 'currenc', 'drone', 'rajouri', 'district', 'dilbag', 'singh', 'saturday', 'say', 'announc', 'arrest', 'terrorist', 'pick', 'consign', 'near', 'singh', 'say', 'major', 'success', 'achiev', 'friday', 'even', 'follow', 'coordin', 'joint', 'oper', 'polic', 'rashtriya', 'rifl', 'decis', 'stop', 'print', 'note', 'financ', 'ministri', 'saturday', 'inform', 'sabha', 'decis', 'take', 'discontinu', 'print', 'denomin', 'currenc', 'note', 'minist', 'state', 'financ', 'anurag', 'thakur', 'say', 'print', 'bank', 'note', 'particular', 'denomin', 'decid', 'govern', 'consult', 'maintain', 'desir', 'denomin', 'bengal', 'home', 'illeg', 'bomb', 'make', 'west', 'bengal', 'home', 'illeg', 'bomb', 'make', 'state', 'administr', 'escap', 'account', 'alarm', 'declin', 'order', 'governor', 'jagdeep', 'dhankhar', 'say', 'saturday', 'governor', 'comment', 'come', 'arrest', 'terrorist', 'associ', 'sponsor', 'modul', 'qaida', 'locat', 'west', 'bengal', 'kerala', 'drug', 'case', 'akul', 'aryann', 'leav', 'grill', 'hand', 'phone', 'investig', 'thunderstorm', 'part', 'india', 'moder', 'thunderstorm', 'lightn', 'like', 'isol', 'place', 'countri', 'hour', 'india', 'meteorolog', 'depart', 'say', 'saturday', 'afternoon', 'thunderstorm', 'lightn', 'expect', 'occur', 'odisha', 'west', 'bengal', 'uttar', 'pradesh', 'madhya', 'pradesh', 'karnataka', 'telangana', 'andhra', 'pradesh', 'place', 'viraat', 'decommiss', 'sell', 'scrap', 'indian', 'navi', 'decommiss', 'aircraft', 'carrier', 'viraat', 'saturday', 'sail', 'time', 'alang', 'gujarat', 'break', 'sell', 'scrap', 'viraat', 'begin', 'final', 'journey', 'naval', 'dockyard', 'navi', 'helicopt', 'circl', 'overhead', 'provid', 'majest', 'backdrop', 'vessel', 'voyag', 'order', 'attach', 'hous', 'arrest', 'terrorist', 'saturday', 'order', 'attach', 'properti', 'jaish', 'moham', 'terrorist', 'irshad', 'ahm', 'reshi', 'arrest', 'agenc', 'connect', 'attack', 'crpf', 'group', 'centr', 'south', 'kashmir', 'properti', 'hous', 'irshad', 'reshi', 'father', 'nazir', 'ahmad', 'reshi', 'pulwama', 'district', 'jammu', 'kashmir', 'balloon', 'explod', 'birthday', 'celebr', 'peopl', 'suffer', 'burn', 'injuri', 'chennai', 'friday', 'hydrogen', 'fill', 'balloon', 'explod', 'gather', 'arrang', 'worker', 'celebr', 'prime', 'minist', 'narendra', 'modi', 'birthday', 'function', 'hold', 'obtain', 'permiss', 'korattur', 'polic', 'slap', 'case', 'organ']]\n"
     ]
    }
   ],
   "source": [
    "print(processed_docs[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag Of Words Featurisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bag of words on the data set\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(1727 unique tokens: ['absolut', 'abus', 'accord', 'accus', 'act']...)\n"
     ]
    }
   ],
   "source": [
    "print (dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 absolut\n",
      "1 abus\n",
      "2 accord\n",
      "3 accus\n",
      "4 act\n",
      "5 action\n",
      "6 activ\n",
      "7 actor\n",
      "8 administ\n",
      "9 agenc\n",
      "10 ahead\n",
      "11 alert\n",
      "12 alleg\n",
      "13 amend\n",
      "14 anurag\n",
      "15 area\n",
      "16 armi\n",
      "17 arrang\n",
      "18 arrest\n",
      "19 arunach\n",
      "20 asapila\n"
     ]
    }
   ],
   "source": [
    "#Testing! if the  dictionary created succesfully\n",
    "count=0\n",
    "for k,v in dictionary.iteritems():\n",
    "    print (k, v)\n",
    "    count +=1\n",
    "    if count >20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(1727 unique tokens: ['absolut', 'abus', 'accord', 'accus', 'act']...)\n"
     ]
    }
   ],
   "source": [
    "print (dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the Bag-of-words model for each document i.e for each document we create a dictionary reporting how many\n",
    "#words and how many times those words appear. Save this to 'bow_corpus'\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bow_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 9 (\"agenc\") appears 1 time.\n",
      "Word 54 (\"come\") appears 2 time.\n",
      "Word 59 (\"conduct\") appears 1 time.\n",
      "Word 114 (\"futur\") appears 1 time.\n",
      "Word 134 (\"hold\") appears 1 time.\n",
      "Word 141 (\"indian\") appears 2 time.\n",
      "Word 147 (\"intern\") appears 1 time.\n",
      "Word 168 (\"level\") appears 1 time.\n",
      "Word 215 (\"plan\") appears 1 time.\n",
      "Word 218 (\"point\") appears 1 time.\n",
      "Word 256 (\"say\") appears 1 time.\n",
      "Word 257 (\"school\") appears 1 time.\n",
      "Word 266 (\"serv\") appears 1 time.\n",
      "Word 298 (\"tell\") appears 1 time.\n",
      "Word 335 (\"work\") appears 1 time.\n",
      "Word 339 (\"year\") appears 1 time.\n",
      "Word 392 (\"father\") appears 2 time.\n",
      "Word 444 (\"place\") appears 1 time.\n",
      "Word 458 (\"south\") appears 2 time.\n",
      "Word 462 (\"telangana\") appears 1 time.\n",
      "Word 477 (\"news\") appears 1 time.\n",
      "Word 479 (\"thing\") appears 1 time.\n",
      "Word 485 (\"continu\") appears 1 time.\n",
      "Word 497 (\"add\") appears 1 time.\n",
      "Word 530 (\"imag\") appears 1 time.\n",
      "Word 610 (\"encourag\") appears 1 time.\n",
      "Word 625 (\"food\") appears 1 time.\n",
      "Word 709 (\"student\") appears 1 time.\n",
      "Word 729 (\"want\") appears 3 time.\n",
      "Word 737 (\"adeena\") appears 1 time.\n",
      "Word 738 (\"appreci\") appears 1 time.\n",
      "Word 739 (\"artist\") appears 1 time.\n",
      "Word 740 (\"artwork\") appears 1 time.\n",
      "Word 741 (\"ashna\") appears 5 time.\n",
      "Word 742 (\"border\") appears 1 time.\n",
      "Word 743 (\"british\") appears 2 time.\n",
      "Word 744 (\"businessman\") appears 1 time.\n",
      "Word 745 (\"buy\") appears 1 time.\n",
      "Word 746 (\"canva\") appears 1 time.\n",
      "Word 747 (\"class\") appears 1 time.\n",
      "Word 748 (\"credit\") appears 1 time.\n",
      "Word 749 (\"cultur\") appears 2 time.\n",
      "Word 750 (\"daughter\") appears 1 time.\n",
      "Word 751 (\"decor\") appears 1 time.\n",
      "Word 752 (\"display\") appears 1 time.\n",
      "Word 753 (\"exhibit\") appears 2 time.\n",
      "Word 754 (\"explor\") appears 1 time.\n",
      "Word 755 (\"facebook\") appears 5 time.\n",
      "Word 756 (\"feet\") appears 1 time.\n",
      "Word 757 (\"get\") appears 1 time.\n",
      "Word 758 (\"heritag\") appears 1 time.\n",
      "Word 759 (\"hyderabad\") appears 1 time.\n",
      "Word 760 (\"hyderabadi\") appears 1 time.\n",
      "Word 761 (\"impress\") appears 1 time.\n",
      "Word 762 (\"kitchen\") appears 1 time.\n",
      "Word 763 (\"know\") appears 1 time.\n",
      "Word 764 (\"london\") appears 2 time.\n",
      "Word 765 (\"name\") appears 1 time.\n",
      "Word 766 (\"paint\") appears 8 time.\n",
      "Word 767 (\"proud\") appears 1 time.\n",
      "Word 768 (\"recognis\") appears 1 time.\n",
      "Word 769 (\"recognit\") appears 1 time.\n",
      "Word 770 (\"religion\") appears 1 time.\n",
      "Word 771 (\"restaur\") appears 2 time.\n",
      "Word 772 (\"schoolgirl\") appears 1 time.\n",
      "Word 773 (\"seri\") appears 3 time.\n",
      "Word 774 (\"subject\") appears 1 time.\n",
      "Word 775 (\"support\") appears 1 time.\n",
      "Word 776 (\"syeda\") appears 1 time.\n",
      "Word 777 (\"talent\") appears 1 time.\n",
      "Word 778 (\"tall\") appears 1 time.\n",
      "Word 779 (\"till\") appears 1 time.\n",
      "Word 780 (\"today\") appears 1 time.\n",
      "Word 781 (\"turabi\") appears 2 time.\n",
      "Word 782 (\"upload\") appears 1 time.\n",
      "Word 783 (\"urooj\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "#preview \n",
    "document_num = 10\n",
    "bow_doc_x = bow_corpus[document_num]\n",
    "\n",
    "for i in range(len(bow_doc_x)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_x[i][0], \n",
    "                                                     dictionary[bow_doc_x[i][0]],\n",
    "                                                     bow_doc_x[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying LDA modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                   num_topics = 10, \n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = 10,\n",
    "                                   workers = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.034*\"oneplus\" + 0.015*\"price\" + 0.014*\"list\" + 0.013*\"best\" + 0.013*\"month\" + 0.013*\"seri\" + 0.012*\"year\" + 0.011*\"updat\" + 0.009*\"nord\" + 0.008*\"movi\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.021*\"releas\" + 0.021*\"film\" + 0.018*\"best\" + 0.011*\"list\" + 0.010*\"ethylen\" + 0.010*\"dioxid\" + 0.010*\"carbon\" + 0.010*\"reaction\" + 0.010*\"year\" + 0.009*\"date\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.020*\"say\" + 0.014*\"polic\" + 0.011*\"paint\" + 0.011*\"peopl\" + 0.010*\"chines\" + 0.010*\"simmon\" + 0.007*\"india\" + 0.007*\"facebook\" + 0.007*\"ashna\" + 0.006*\"school\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.001*\"oneplus\" + 0.001*\"year\" + 0.001*\"say\" + 0.001*\"price\" + 0.001*\"cinema\" + 0.001*\"help\" + 0.001*\"saturday\" + 0.001*\"malayalam\" + 0.001*\"india\" + 0.001*\"month\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.020*\"latest\" + 0.014*\"movi\" + 0.014*\"road\" + 0.010*\"cinema\" + 0.009*\"happen\" + 0.009*\"know\" + 0.009*\"announc\" + 0.008*\"daili\" + 0.008*\"want\" + 0.008*\"accid\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.012*\"india\" + 0.009*\"say\" + 0.009*\"agricultur\" + 0.008*\"saturday\" + 0.007*\"indian\" + 0.006*\"nation\" + 0.005*\"like\" + 0.005*\"bengal\" + 0.005*\"swaminathan\" + 0.005*\"peopl\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.028*\"movi\" + 0.016*\"london\" + 0.016*\"pandem\" + 0.011*\"post\" + 0.011*\"world\" + 0.009*\"avail\" + 0.009*\"countri\" + 0.009*\"content\" + 0.009*\"sorri\" + 0.009*\"thriller\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.022*\"student\" + 0.019*\"know\" + 0.016*\"play\" + 0.016*\"choic\" + 0.016*\"devgn\" + 0.016*\"ajay\" + 0.016*\"citi\" + 0.016*\"smartphon\" + 0.013*\"school\" + 0.010*\"best\"\n",
      "\n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.027*\"aashram\" + 0.020*\"player\" + 0.011*\"come\" + 0.011*\"baba\" + 0.009*\"get\" + 0.009*\"deol\" + 0.009*\"bobbi\" + 0.008*\"seri\" + 0.008*\"show\" + 0.008*\"skeleton\"\n",
      "\n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.038*\"websit\" + 0.019*\"best\" + 0.019*\"chang\" + 0.019*\"updat\" + 0.019*\"union\" + 0.019*\"review\" + 0.019*\"accept\" + 0.019*\"continu\" + 0.019*\"polici\" + 0.019*\"ensur\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics():\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Topic Modeling on the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list = site.articles[51:60]\n",
    "article_list_test=[]\n",
    "for i in article_list:\n",
    "    i.download()\n",
    "    i.parse()\n",
    "    article_list_test.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Rail Land Development Authority (RLDA) has invited bids from private players to redevelop an area of five lakh square metres at the New Delhi station and another 2.6 lakh square metres surrounding it for commercial purposes. The organisation said that the project was likely to be completed in around four years. The plan is to redevelop the iconic station, road connections through flyovers, relocation of railway offices and buildings and creating social infrastructure. The estimated cost of the entire project is Rs 6,500 crore.'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_list_test[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.08916326612234116\t Topic: 0.034*\"oneplus\" + 0.015*\"price\" + 0.014*\"list\" + 0.013*\"best\" + 0.013*\"month\" + 0.013*\"seri\" + 0.012*\"year\" + 0.011*\"updat\" + 0.009*\"nord\" + 0.008*\"movi\" + 0.008*\"bengali\" + 0.008*\"discount\" + 0.008*\"cost\" + 0.008*\"sale\" + 0.008*\"option\" + 0.008*\"flagship\" + 0.008*\"offer\" + 0.008*\"content\" + 0.008*\"right\" + 0.007*\"time\" + 0.006*\"decis\" + 0.006*\"train\" + 0.006*\"smartphon\" + 0.006*\"storag\" + 0.006*\"season\" + 0.006*\"drop\" + 0.006*\"deal\" + 0.006*\"attent\" + 0.005*\"cast\" + 0.005*\"rat\"\n",
      "Score: 0.11471286416053772\t Topic: 0.020*\"say\" + 0.014*\"polic\" + 0.011*\"paint\" + 0.011*\"peopl\" + 0.010*\"chines\" + 0.010*\"simmon\" + 0.007*\"india\" + 0.007*\"facebook\" + 0.007*\"ashna\" + 0.006*\"school\" + 0.006*\"come\" + 0.006*\"larg\" + 0.006*\"sharma\" + 0.006*\"intellig\" + 0.006*\"arrest\" + 0.006*\"parti\" + 0.004*\"indian\" + 0.004*\"educ\" + 0.004*\"seri\" + 0.004*\"want\" + 0.004*\"hold\" + 0.004*\"nation\" + 0.004*\"kumar\" + 0.004*\"document\" + 0.004*\"inform\" + 0.004*\"delhi\" + 0.004*\"citi\" + 0.004*\"special\" + 0.004*\"cell\" + 0.004*\"classifi\"\n",
      "Score: 0.18619458377361298\t Topic: 0.020*\"latest\" + 0.014*\"movi\" + 0.014*\"road\" + 0.010*\"cinema\" + 0.009*\"happen\" + 0.009*\"know\" + 0.009*\"announc\" + 0.008*\"daili\" + 0.008*\"want\" + 0.008*\"accid\" + 0.008*\"news\" + 0.007*\"updat\" + 0.006*\"read\" + 0.006*\"inform\" + 0.006*\"stop\" + 0.005*\"star\" + 0.005*\"place\" + 0.005*\"tamil\" + 0.005*\"curious\" + 0.005*\"click\" + 0.005*\"pictur\" + 0.005*\"lakh\" + 0.005*\"district\" + 0.005*\"safeti\" + 0.005*\"good\" + 0.005*\"plot\" + 0.005*\"follow\" + 0.005*\"bollywood\" + 0.005*\"look\" + 0.004*\"peopl\"\n",
      "Score: 0.4825863540172577\t Topic: 0.012*\"india\" + 0.009*\"say\" + 0.009*\"agricultur\" + 0.008*\"saturday\" + 0.007*\"indian\" + 0.006*\"nation\" + 0.005*\"like\" + 0.005*\"bengal\" + 0.005*\"swaminathan\" + 0.005*\"peopl\" + 0.005*\"state\" + 0.005*\"year\" + 0.005*\"pradesh\" + 0.005*\"arrest\" + 0.004*\"hear\" + 0.004*\"debut\" + 0.004*\"time\" + 0.004*\"make\" + 0.004*\"research\" + 0.004*\"star\" + 0.004*\"yield\" + 0.003*\"movi\" + 0.003*\"sandalwood\" + 0.003*\"happi\" + 0.003*\"help\" + 0.003*\"terrorist\" + 0.003*\"farmer\" + 0.003*\"field\" + 0.003*\"world\" + 0.003*\"case\"\n",
      "Score: 0.038611605763435364\t Topic: 0.022*\"student\" + 0.019*\"know\" + 0.016*\"play\" + 0.016*\"choic\" + 0.016*\"devgn\" + 0.016*\"ajay\" + 0.016*\"citi\" + 0.016*\"smartphon\" + 0.013*\"school\" + 0.010*\"best\" + 0.010*\"india\" + 0.010*\"live\" + 0.010*\"teacher\" + 0.007*\"indian\" + 0.007*\"express\" + 0.007*\"contribut\" + 0.007*\"govern\" + 0.007*\"selvaraj\" + 0.007*\"bairavi\" + 0.007*\"tamilselvan\" + 0.007*\"distribut\" + 0.003*\"say\" + 0.003*\"come\" + 0.003*\"develop\" + 0.003*\"state\" + 0.003*\"process\" + 0.003*\"time\" + 0.003*\"show\" + 0.003*\"scienc\" + 0.003*\"survey\"\n",
      "Score: 0.07539619505405426\t Topic: 0.027*\"aashram\" + 0.020*\"player\" + 0.011*\"come\" + 0.011*\"baba\" + 0.009*\"get\" + 0.009*\"deol\" + 0.009*\"bobbi\" + 0.008*\"seri\" + 0.008*\"show\" + 0.008*\"skeleton\" + 0.008*\"kashipur\" + 0.007*\"prakash\" + 0.007*\"power\" + 0.007*\"play\" + 0.007*\"charact\" + 0.007*\"watch\" + 0.006*\"case\" + 0.006*\"manipul\" + 0.006*\"true\" + 0.006*\"justic\" + 0.006*\"pammi\" + 0.006*\"pretti\" + 0.006*\"waal\" + 0.006*\"polit\" + 0.006*\"darshan\" + 0.006*\"kumaar\" + 0.006*\"perfect\" + 0.005*\"peopl\" + 0.005*\"perform\" + 0.004*\"look\"\n"
     ]
    }
   ],
   "source": [
    "bow_vector = dictionary.doc2bow(preprocess(article_list_test[8]))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
